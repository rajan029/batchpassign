{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae9d9622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "### Part 1: Understanding Batch Normalization\n",
    "\n",
    "#Q1a. Explain the Concept of Batch Normalization**\n",
    "\n",
    "#Batch normalization is a technique used to improve the training of deep neural networks by normalizing the inputs of each layer. It standardizes the inputs to a layer for each mini-batch, maintaining the mean output close to 0 and the output standard deviation close to 1. This helps in stabilizing the learning process and often results in faster convergence.\n",
    "'''\n",
    "Q1b. Describe the Benefits of Using Batch Normalization During Training**\n",
    "\n",
    "- **Stabilizes Training**: By normalizing the inputs, it reduces the problem of internal covariate shift, making the training more stable.\n",
    "- **Accelerates Convergence**: Normalized inputs allow for higher learning rates without the risk of divergence.\n",
    "- **Regularization**: Acts as a form of regularization, reducing the need for other regularization methods like dropout.\n",
    "- **Reduces Dependency on Initialization**: Less sensitivity to the initial weights, which means the network can be trained with different initializations and still converge effectively.\n",
    "'''\n",
    "#Q1c. Discuss the Working Principle of Batch Normalization**\n",
    "\n",
    "### Part 2: Implementation\n",
    "\n",
    "#Q2a. Choose a Dataset and Preprocess It**\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess the CIFAR-10 dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Normalize the input data\n",
    "X_train = X_train.astype('float32') / 255.0\n",
    "X_test = X_test.astype('float32') / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "\n",
    "#Q2b. Implement a Simple Feedforward Neural Network Without Batch Normalization**\n",
    "\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "\n",
    "# Define the model\n",
    "model_without_bn = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_without_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_without_bn = model_without_bn.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=64)\n",
    "\n",
    "\n",
    "#Q2c. Implement Batch Normalization Layers in the Neural Network**\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Define the model with batch normalization\n",
    "model_with_bn = Sequential([\n",
    "    Flatten(input_shape=(32, 32, 3)),\n",
    "    Dense(512, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model_with_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history_with_bn = model_with_bn.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=64)\n",
    "\n",
    "#Q2d. Compare Training and Validation Performance**\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history_without_bn.history['accuracy'], label='Train Accuracy without BN')\n",
    "plt.plot(history_without_bn.history['val_accuracy'], label='Val Accuracy without BN')\n",
    "plt.plot(history_with_bn.history['accuracy'], label='Train Accuracy with BN')\n",
    "plt.plot(history_with_bn.history['val_accuracy'], label='Val Accuracy with BN')\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history_without_bn.history['loss'], label='Train Loss without BN')\n",
    "plt.plot(history_without_bn.history['val_loss'], label='Val Loss without BN')\n",
    "plt.plot(history_with_bn.history['loss'], label='Train Loss with BN')\n",
    "plt.plot(history_with_bn.history['val_loss'], label='Val Loss with BN')\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Q2e. Discuss the Impact of Batch Normalization on Training Process and Performance**\n",
    "\n",
    "#Batch normalization typically improves both training speed and model performance. From the plots, we can observe that the model with batch normalization converges faster and achieves higher accuracy on both training and validation sets.\n",
    "\n",
    "### Part 3: Experimentation and Analysis\n",
    "\n",
    "#Q3a. Experiment with Different Batch Sizes**\n",
    "\n",
    "\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [32, 64, 128]\n",
    "histories = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    model_with_bn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    history = model_with_bn.fit(X_train, y_train, validation_split=0.2, epochs=20, batch_size=batch_size)\n",
    "    histories.append(history)\n",
    "\n",
    "'''\n",
    "Q3b. Discuss Advantages and Potential Limitations of Batch Normalization**\n",
    "\n",
    "**Advantages:**\n",
    "- Speeds up training.\n",
    "- Improves model performance.\n",
    "- Reduces sensitivity to weight initialization.\n",
    "- Provides some regularization.\n",
    "\n",
    "**Potential Limitations:**\n",
    "- Adds computational overhead.\n",
    "- Can interact with dropout in complex ways.\n",
    "- May not be effective for very small mini-batches.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ed16bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
